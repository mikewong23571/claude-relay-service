const express = require('express')
const axios = require('axios')
const router = express.Router()
const logger = require('../utils/logger')
const config = require('../../config/config')
const { authenticateApiKey } = require('../middleware/auth')
const unifiedOpenAIScheduler = require('../services/unifiedOpenAIScheduler')
const openaiAccountService = require('../services/openaiAccountService')
const openaiResponsesAccountService = require('../services/openaiResponsesAccountService')
const openaiResponsesRelayService = require('../services/openaiResponsesRelayService')
const apiKeyService = require('../services/apiKeyService')
const crypto = require('crypto')
const ProxyHelper = require('../utils/proxyHelper')
const { updateRateLimitCounters } = require('../utils/rateLimitHelper')

// åˆ›å»ºä»£ç† Agentï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ä»£ç†å·¥å…·ï¼‰
function createProxyAgent(proxy) {
  return ProxyHelper.createProxyAgent(proxy)
}

// æ£€æŸ¥ API Key æ˜¯å¦å…·å¤‡ OpenAI æƒé™
function checkOpenAIPermissions(apiKeyData) {
  const permissions = apiKeyData?.permissions || 'all'
  return permissions === 'all' || permissions === 'openai'
}

function normalizeHeaders(headers = {}) {
  if (!headers || typeof headers !== 'object') {
    return {}
  }
  const normalized = {}
  for (const [key, value] of Object.entries(headers)) {
    if (!key) {
      continue
    }
    normalized[key.toLowerCase()] = Array.isArray(value) ? value[0] : value
  }
  return normalized
}

function toNumberSafe(value) {
  if (value === undefined || value === null || value === '') {
    return null
  }
  const num = Number(value)
  return Number.isFinite(num) ? num : null
}

function extractCodexUsageHeaders(headers) {
  const normalized = normalizeHeaders(headers)
  if (!normalized || Object.keys(normalized).length === 0) {
    return null
  }

  const snapshot = {
    primaryUsedPercent: toNumberSafe(normalized['x-codex-primary-used-percent']),
    primaryResetAfterSeconds: toNumberSafe(normalized['x-codex-primary-reset-after-seconds']),
    primaryWindowMinutes: toNumberSafe(normalized['x-codex-primary-window-minutes']),
    secondaryUsedPercent: toNumberSafe(normalized['x-codex-secondary-used-percent']),
    secondaryResetAfterSeconds: toNumberSafe(normalized['x-codex-secondary-reset-after-seconds']),
    secondaryWindowMinutes: toNumberSafe(normalized['x-codex-secondary-window-minutes']),
    primaryOverSecondaryPercent: toNumberSafe(
      normalized['x-codex-primary-over-secondary-limit-percent']
    )
  }

  const hasData = Object.values(snapshot).some((value) => value !== null)
  return hasData ? snapshot : null
}

async function applyRateLimitTracking(req, usageSummary, model, context = '') {
  if (!req.rateLimitInfo) {
    return
  }

  const label = context ? ` (${context})` : ''

  try {
    const { totalTokens, totalCost } = await updateRateLimitCounters(
      req.rateLimitInfo,
      usageSummary,
      model
    )

    if (totalTokens > 0) {
      logger.api(`ðŸ“Š Updated rate limit token count${label}: +${totalTokens} tokens`)
    }
    if (typeof totalCost === 'number' && totalCost > 0) {
      logger.api(`ðŸ’° Updated rate limit cost count${label}: +$${totalCost.toFixed(6)}`)
    }
  } catch (error) {
    logger.error(`âŒ Failed to update rate limit counters${label}:`, error)
  }
}

// ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦å™¨é€‰æ‹© OpenAI è´¦æˆ·
async function getOpenAIAuthToken(apiKeyData, sessionId = null, requestedModel = null) {
  try {
    // ç”Ÿæˆä¼šè¯å“ˆå¸Œï¼ˆå¦‚æžœæœ‰ä¼šè¯IDï¼‰
    const sessionHash = sessionId
      ? crypto.createHash('sha256').update(sessionId).digest('hex')
      : null

    // ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦å™¨é€‰æ‹©è´¦æˆ·
    const result = await unifiedOpenAIScheduler.selectAccountForApiKey(
      apiKeyData,
      sessionHash,
      requestedModel
    )

    if (!result || !result.accountId) {
      const error = new Error('No available OpenAI account found')
      error.statusCode = 402 // Payment Required - èµ„æºè€—å°½
      throw error
    }

    // æ ¹æ®è´¦æˆ·ç±»åž‹èŽ·å–è´¦æˆ·è¯¦æƒ…
    let account,
      accessToken,
      proxy = null

    if (result.accountType === 'openai-responses') {
      // å¤„ç† OpenAI-Responses è´¦æˆ·
      account = await openaiResponsesAccountService.getAccount(result.accountId)
      if (!account || !account.apiKey) {
        const error = new Error(`OpenAI-Responses account ${result.accountId} has no valid apiKey`)
        error.statusCode = 403 // Forbidden - è´¦æˆ·é…ç½®é”™è¯¯
        throw error
      }

      // OpenAI-Responses è´¦æˆ·ä¸éœ€è¦ accessTokenï¼Œç›´æŽ¥è¿”å›žè´¦æˆ·ä¿¡æ¯
      accessToken = null // OpenAI-Responses ä½¿ç”¨è´¦æˆ·å†…çš„ apiKey

      // è§£æžä»£ç†é…ç½®
      if (account.proxy) {
        try {
          proxy = typeof account.proxy === 'string' ? JSON.parse(account.proxy) : account.proxy
        } catch (e) {
          logger.warn('Failed to parse proxy configuration:', e)
        }
      }

      logger.info(`Selected OpenAI-Responses account: ${account.name} (${result.accountId})`)
    } else {
      // å¤„ç†æ™®é€š OpenAI è´¦æˆ·
      account = await openaiAccountService.getAccount(result.accountId)
      if (!account || !account.accessToken) {
        const error = new Error(`OpenAI account ${result.accountId} has no valid accessToken`)
        error.statusCode = 403 // Forbidden - è´¦æˆ·é…ç½®é”™è¯¯
        throw error
      }

      // æ£€æŸ¥ token æ˜¯å¦è¿‡æœŸå¹¶è‡ªåŠ¨åˆ·æ–°ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
      if (openaiAccountService.isTokenExpired(account)) {
        if (account.refreshToken) {
          logger.info(`ðŸ”„ Token expired, auto-refreshing for account ${account.name} (fallback)`)
          try {
            await openaiAccountService.refreshAccountToken(result.accountId)
            // é‡æ–°èŽ·å–æ›´æ–°åŽçš„è´¦æˆ·
            account = await openaiAccountService.getAccount(result.accountId)
            logger.info(`âœ… Token refreshed successfully in route handler`)
          } catch (refreshError) {
            logger.error(`Failed to refresh token for ${account.name}:`, refreshError)
            const error = new Error(`Token expired and refresh failed: ${refreshError.message}`)
            error.statusCode = 403 // Forbidden - è®¤è¯å¤±è´¥
            throw error
          }
        } else {
          const error = new Error(
            `Token expired and no refresh token available for account ${account.name}`
          )
          error.statusCode = 403 // Forbidden - è®¤è¯å¤±è´¥
          throw error
        }
      }

      // è§£å¯† accessTokenï¼ˆaccount.accessToken æ˜¯åŠ å¯†çš„ï¼‰
      accessToken = openaiAccountService.decrypt(account.accessToken)
      if (!accessToken) {
        const error = new Error('Failed to decrypt OpenAI accessToken')
        error.statusCode = 403 // Forbidden - é…ç½®/æƒé™é”™è¯¯
        throw error
      }

      // è§£æžä»£ç†é…ç½®
      if (account.proxy) {
        try {
          proxy = typeof account.proxy === 'string' ? JSON.parse(account.proxy) : account.proxy
        } catch (e) {
          logger.warn('Failed to parse proxy configuration:', e)
        }
      }

      logger.info(`Selected OpenAI account: ${account.name} (${result.accountId})`)
    }

    return {
      accessToken,
      accountId: result.accountId,
      accountName: account.name,
      accountType: result.accountType,
      proxy,
      account
    }
  } catch (error) {
    logger.error('Failed to get OpenAI auth token:', error)
    throw error
  }
}

// ä¸»å¤„ç†å‡½æ•°ï¼Œä¾›ä¸¤ä¸ªè·¯ç”±å…±äº«
const handleResponses = async (req, res) => {
  let upstream = null
  let accountId = null
  let accountType = 'openai'
  let sessionHash = null
  let account = null
  let proxy = null
  let accessToken = null

  try {
    // ä»Žä¸­é—´ä»¶èŽ·å– API Key æ•°æ®
    const apiKeyData = req.apiKey || {}

    if (!checkOpenAIPermissions(apiKeyData)) {
      logger.security(
        `ðŸš« API Key ${apiKeyData.id || 'unknown'} ç¼ºå°‘ OpenAI æƒé™ï¼Œæ‹’ç»è®¿é—® ${req.originalUrl}`
      )
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access OpenAI',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }

    // ä»Žè¯·æ±‚å¤´æˆ–è¯·æ±‚ä½“ä¸­æå–ä¼šè¯ ID
    const sessionId =
      req.headers['session_id'] ||
      req.headers['x-session-id'] ||
      req.body?.session_id ||
      req.body?.conversation_id ||
      null

    sessionHash = sessionId ? crypto.createHash('sha256').update(sessionId).digest('hex') : null

    // ä»Žè¯·æ±‚ä½“ä¸­æå–æ¨¡åž‹å’Œæµå¼æ ‡å¿—
    let requestedModel = req.body?.model || null

    // å¦‚æžœæ¨¡åž‹æ˜¯ gpt-5 å¼€å¤´ä¸”åŽé¢è¿˜æœ‰å†…å®¹ï¼ˆå¦‚ gpt-5-2025-08-07ï¼‰ï¼Œåˆ™è¦†ç›–ä¸º gpt-5
    if (requestedModel && requestedModel.startsWith('gpt-5-') && requestedModel !== 'gpt-5-codex') {
      logger.info(`ðŸ“ Model ${requestedModel} detected, normalizing to gpt-5 for Codex API`)
      requestedModel = 'gpt-5'
      req.body.model = 'gpt-5' // åŒæ—¶æ›´æ–°è¯·æ±‚ä½“ä¸­çš„æ¨¡åž‹
    }

    const isStream = req.body?.stream !== false // é»˜è®¤ä¸ºæµå¼ï¼ˆå…¼å®¹çŽ°æœ‰è¡Œä¸ºï¼‰

    // ä½¿ç”¨è°ƒåº¦å™¨é€‰æ‹©è´¦æˆ·
    ; ({ accessToken, accountId, accountType, proxy, account } = await getOpenAIAuthToken(
      apiKeyData,
      sessionId,
      requestedModel
    ))

    // å¦‚æžœæ˜¯ OpenAI-Responses è´¦æˆ·ï¼Œä½¿ç”¨ä¸“é—¨çš„ä¸­ç»§æœåŠ¡å¤„ç†
    if (accountType === 'openai-responses') {
      logger.info(`ðŸ”€ Using OpenAI-Responses relay service for account: ${account.name}`)
      return await openaiResponsesRelayService.handleRequest(req, res, account, apiKeyData)
    }
    // åŸºäºŽç™½åå•æž„é€ ä¸Šæ¸¸æ‰€éœ€çš„è¯·æ±‚å¤´ï¼Œç¡®ä¿é”®ä¸ºå°å†™ä¸”å€¼å—æŽ§
    const incoming = req.headers || {}

    const allowedKeys = ['version', 'openai-beta', 'session_id']

    const headers = {}
    for (const key of allowedKeys) {
      if (incoming[key] !== undefined) {
        headers[key] = incoming[key]
      }
    }

    // åˆ¤æ–­æ˜¯å¦è®¿é—® compact ç«¯ç‚¹
    const isCompactRoute =
      req.path === '/responses/compact' ||
      req.path === '/v1/responses/compact' ||
      (req.originalUrl && req.originalUrl.includes('/responses/compact'))

    // è¦†ç›–æˆ–æ–°å¢žå¿…è¦å¤´éƒ¨
    headers['authorization'] = `Bearer ${accessToken}`
    headers['chatgpt-account-id'] = account.accountId || account.chatgptUserId || accountId
    headers['host'] = 'chatgpt.com'
    headers['accept'] = isStream ? 'text/event-stream' : 'application/json'
    headers['content-type'] = 'application/json'
    if (!isCompactRoute) {
      req.body['store'] = false
    } else if (req.body && Object.prototype.hasOwnProperty.call(req.body, 'store')) {
      delete req.body['store']
    }

    // åˆ›å»ºä»£ç† agent
    const proxyAgent = createProxyAgent(proxy)

    // é…ç½®è¯·æ±‚é€‰é¡¹
    const axiosConfig = {
      headers,
      timeout: config.requestTimeout || 600000,
      validateStatus: () => true
    }

    // å¦‚æžœæœ‰ä»£ç†ï¼Œæ·»åŠ ä»£ç†é…ç½®
    if (proxyAgent) {
      axiosConfig.httpAgent = proxyAgent
      axiosConfig.httpsAgent = proxyAgent
      axiosConfig.proxy = false
      logger.info(`ðŸŒ Using proxy for OpenAI request: ${ProxyHelper.getProxyDescription(proxy)}`)
    } else {
      logger.debug('ðŸŒ No proxy configured for OpenAI request')
    }

    const codexEndpoint = isCompactRoute
      ? 'https://chatgpt.com/backend-api/codex/responses/compact'
      : 'https://chatgpt.com/backend-api/codex/responses'

    // æ ¹æ® stream å‚æ•°å†³å®šè¯·æ±‚ç±»åž‹
    if (isStream) {
      // æµå¼è¯·æ±‚
      upstream = await axios.post(codexEndpoint, req.body, {
        ...axiosConfig,
        responseType: 'stream'
      })
    } else {
      // éžæµå¼è¯·æ±‚
      upstream = await axios.post(codexEndpoint, req.body, axiosConfig)
    }

    const codexUsageSnapshot = extractCodexUsageHeaders(upstream.headers)
    if (codexUsageSnapshot) {
      try {
        await openaiAccountService.updateCodexUsageSnapshot(accountId, codexUsageSnapshot)
      } catch (codexError) {
        logger.error('âš ï¸ æ›´æ–° Codex ä½¿ç”¨ç»Ÿè®¡å¤±è´¥:', codexError)
      }
    }

    // å¤„ç† 429 é™æµé”™è¯¯
    if (upstream.status === 429) {
      logger.warn(`ðŸš« Rate limit detected for OpenAI account ${accountId} (Codex API)`)

      // è§£æžå“åº”ä½“ä¸­çš„é™æµä¿¡æ¯
      let resetsInSeconds = null
      let errorData = null

      try {
        // å¯¹äºŽ429é”™è¯¯ï¼Œæ— è®ºæ˜¯å¦æ˜¯æµå¼è¯·æ±‚ï¼Œå“åº”éƒ½ä¼šæ˜¯å®Œæ•´çš„JSONé”™è¯¯å¯¹è±¡
        if (isStream && upstream.data) {
          // æµå¼å“åº”éœ€è¦å…ˆæ”¶é›†æ•°æ®
          const chunks = []
          await new Promise((resolve, reject) => {
            upstream.data.on('data', (chunk) => chunks.push(chunk))
            upstream.data.on('end', resolve)
            upstream.data.on('error', reject)
            // è®¾ç½®è¶…æ—¶é˜²æ­¢æ— é™ç­‰å¾…
            setTimeout(resolve, 5000)
          })

          const fullResponse = Buffer.concat(chunks).toString()
          try {
            errorData = JSON.parse(fullResponse)
          } catch (e) {
            logger.error('Failed to parse 429 error response:', e)
            logger.debug('Raw response:', fullResponse)
          }
        } else {
          // éžæµå¼å“åº”ç›´æŽ¥ä½¿ç”¨data
          errorData = upstream.data
        }

        // æå–é‡ç½®æ—¶é—´
        if (errorData && errorData.error && errorData.error.resets_in_seconds) {
          resetsInSeconds = errorData.error.resets_in_seconds
          logger.info(
            `ðŸ• Codex rate limit will reset in ${resetsInSeconds} seconds (${Math.ceil(resetsInSeconds / 60)} minutes / ${Math.ceil(resetsInSeconds / 3600)} hours)`
          )
        } else {
          logger.warn(
            'âš ï¸ Could not extract resets_in_seconds from 429 response, using default 60 minutes'
          )
        }
      } catch (e) {
        logger.error('âš ï¸ Failed to parse rate limit error:', e)
      }

      // æ ‡è®°è´¦æˆ·ä¸ºé™æµçŠ¶æ€
      await unifiedOpenAIScheduler.markAccountRateLimited(
        accountId,
        'openai',
        sessionHash,
        resetsInSeconds
      )

      // è¿”å›žé”™è¯¯å“åº”ç»™å®¢æˆ·ç«¯
      const errorResponse = errorData || {
        error: {
          type: 'usage_limit_reached',
          message: 'The usage limit has been reached',
          resets_in_seconds: resetsInSeconds
        }
      }

      if (isStream) {
        // æµå¼å“åº”ä¹Ÿéœ€è¦è®¾ç½®æ­£ç¡®çš„çŠ¶æ€ç 
        res.status(429)
        res.setHeader('Content-Type', 'text/event-stream')
        res.setHeader('Cache-Control', 'no-cache')
        res.setHeader('Connection', 'keep-alive')
        res.write(`data: ${JSON.stringify(errorResponse)}\n\n`)
        res.end()
      } else {
        res.status(429).json(errorResponse)
      }

      return
    } else if (upstream.status === 401 || upstream.status === 402) {
      const unauthorizedStatus = upstream.status
      const statusDescription = unauthorizedStatus === 401 ? 'Unauthorized' : 'Payment required'
      logger.warn(
        `ðŸ” ${statusDescription} error detected for OpenAI account ${accountId} (Codex API)`
      )

      let errorData = null

      try {
        if (isStream && upstream.data && typeof upstream.data.on === 'function') {
          const chunks = []
          await new Promise((resolve, reject) => {
            upstream.data.on('data', (chunk) => chunks.push(chunk))
            upstream.data.on('end', resolve)
            upstream.data.on('error', reject)
            setTimeout(resolve, 5000)
          })

          const fullResponse = Buffer.concat(chunks).toString()
          try {
            errorData = JSON.parse(fullResponse)
          } catch (parseError) {
            logger.error(`Failed to parse ${unauthorizedStatus} error response:`, parseError)
            logger.debug(`Raw ${unauthorizedStatus} response:`, fullResponse)
            errorData = { error: { message: fullResponse || 'Unauthorized' } }
          }
        } else {
          errorData = upstream.data
        }
      } catch (parseError) {
        logger.error(`âš ï¸ Failed to handle ${unauthorizedStatus} error response:`, parseError)
      }

      const statusLabel = unauthorizedStatus === 401 ? '401é”™è¯¯' : '402é”™è¯¯'
      const extraHint = unauthorizedStatus === 402 ? 'ï¼Œå¯èƒ½æ¬ è´¹' : ''
      let reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰`
      if (errorData) {
        const messageCandidate =
          errorData.error &&
            typeof errorData.error.message === 'string' &&
            errorData.error.message.trim()
            ? errorData.error.message.trim()
            : typeof errorData.message === 'string' && errorData.message.trim()
              ? errorData.message.trim()
              : null
        if (messageCandidate) {
          reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${messageCandidate}`
        }
      }

      try {
        await unifiedOpenAIScheduler.markAccountUnauthorized(
          accountId,
          'openai',
          sessionHash,
          reason
        )
      } catch (markError) {
        logger.error(
          `âŒ Failed to mark OpenAI account unauthorized after ${unauthorizedStatus}:`,
          markError
        )
      }

      let errorResponse = errorData
      if (!errorResponse || typeof errorResponse !== 'object' || Buffer.isBuffer(errorResponse)) {
        const fallbackMessage =
          typeof errorData === 'string' && errorData.trim() ? errorData.trim() : 'Unauthorized'
        errorResponse = {
          error: {
            message: fallbackMessage,
            type: 'unauthorized',
            code: 'unauthorized'
          }
        }
      }

      res.status(unauthorizedStatus).json(errorResponse)
      return
    } else if (upstream.status === 200 || upstream.status === 201) {
      // è¯·æ±‚æˆåŠŸï¼Œæ£€æŸ¥å¹¶ç§»é™¤é™æµçŠ¶æ€
      const isRateLimited = await unifiedOpenAIScheduler.isAccountRateLimited(accountId)
      if (isRateLimited) {
        logger.info(
          `âœ… Removing rate limit for OpenAI account ${accountId} after successful request`
        )
        await unifiedOpenAIScheduler.removeAccountRateLimit(accountId, 'openai')
      }
    }

    res.status(upstream.status)

    if (isStream) {
      // æµå¼å“åº”å¤´
      res.setHeader('Content-Type', 'text/event-stream')
      res.setHeader('Cache-Control', 'no-cache')
      res.setHeader('Connection', 'keep-alive')
      res.setHeader('X-Accel-Buffering', 'no')
    } else {
      // éžæµå¼å“åº”å¤´
      res.setHeader('Content-Type', 'application/json')
    }

    // é€ä¼ å…³é”®è¯Šæ–­å¤´ï¼Œé¿å…ä¼ é€’ä¸å®‰å…¨æˆ–ä¸Žä¼ è¾“ç›¸å…³çš„å¤´
    const passThroughHeaderKeys = ['openai-version', 'x-request-id', 'openai-processing-ms']
    for (const key of passThroughHeaderKeys) {
      const val = upstream.headers?.[key]
      if (val !== undefined) {
        res.setHeader(key, val)
      }
    }

    if (isStream) {
      // ç«‹å³åˆ·æ–°å“åº”å¤´ï¼Œå¼€å§‹ SSE
      if (typeof res.flushHeaders === 'function') {
        res.flushHeaders()
      }
    }

    // å¤„ç†å“åº”å¹¶æ•èŽ· usage æ•°æ®å’ŒçœŸå®žçš„ model
    let buffer = ''
    let usageData = null
    let actualModel = null
    let usageReported = false
    let rateLimitDetected = false
    let rateLimitResetsInSeconds = null

    if (!isStream) {
      // éžæµå¼å“åº”å¤„ç†
      try {
        logger.info(`ðŸ“„ Processing OpenAI non-stream response for model: ${requestedModel}`)

        // ç›´æŽ¥èŽ·å–å®Œæ•´å“åº”
        const responseData = upstream.data

        // ä»Žå“åº”ä¸­èŽ·å–å®žé™…çš„ model å’Œ usage
        actualModel = responseData.model || requestedModel || 'gpt-4'
        usageData = responseData.usage

        logger.debug(`ðŸ“Š Non-stream response - Model: ${actualModel}, Usage:`, usageData)

        // è®°å½•ä½¿ç”¨ç»Ÿè®¡
        if (usageData) {
          const totalInputTokens = usageData.input_tokens || usageData.prompt_tokens || 0
          const outputTokens = usageData.output_tokens || usageData.completion_tokens || 0
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0
          // è®¡ç®—å®žé™…è¾“å…¥tokenï¼ˆæ€»è¾“å…¥å‡åŽ»ç¼“å­˜éƒ¨åˆ†ï¼‰
          const actualInputTokens = Math.max(0, totalInputTokens - cacheReadTokens)

          await apiKeyService.recordUsage(
            apiKeyData.id,
            actualInputTokens, // ä¼ é€’å®žé™…è¾“å…¥ï¼ˆä¸å«ç¼“å­˜ï¼‰
            outputTokens,
            0, // OpenAIæ²¡æœ‰cache_creation_tokens
            cacheReadTokens,
            actualModel,
            accountId
          )

          logger.info(
            `ðŸ“Š Recorded OpenAI non-stream usage - Input: ${totalInputTokens}(actual:${actualInputTokens}+cached:${cacheReadTokens}), Output: ${outputTokens}, Total: ${usageData.total_tokens || totalInputTokens + outputTokens}, Model: ${actualModel}`
          )

          await applyRateLimitTracking(
            req,
            {
              inputTokens: actualInputTokens,
              outputTokens,
              cacheCreateTokens: 0,
              cacheReadTokens
            },
            actualModel,
            'openai-non-stream'
          )
        }

        // è¿”å›žå“åº”
        res.json(responseData)
        return
      } catch (error) {
        logger.error('Failed to process non-stream response:', error)
        if (!res.headersSent) {
          res.status(500).json({ error: { message: 'Failed to process response' } })
        }
        return
      }
    }

    // è§£æž SSE äº‹ä»¶ä»¥æ•èŽ· usage æ•°æ®å’Œ model
    const parseSSEForUsage = (data) => {
      const lines = data.split('\n')

      for (const line of lines) {
        if (line.startsWith('event: response.completed')) {
          // ä¸‹ä¸€è¡Œåº”è¯¥æ˜¯æ•°æ®
          continue
        }

        if (line.startsWith('data: ')) {
          try {
            const jsonStr = line.slice(6) // ç§»é™¤ 'data: ' å‰ç¼€
            const eventData = JSON.parse(jsonStr)

            // æ£€æŸ¥æ˜¯å¦æ˜¯ response.completed äº‹ä»¶
            if (eventData.type === 'response.completed' && eventData.response) {
              // ä»Žå“åº”ä¸­èŽ·å–çœŸå®žçš„ model
              if (eventData.response.model) {
                actualModel = eventData.response.model
                logger.debug(`ðŸ“Š Captured actual model: ${actualModel}`)
              }

              // èŽ·å– usage æ•°æ®
              if (eventData.response.usage) {
                usageData = eventData.response.usage
                logger.debug('ðŸ“Š Captured OpenAI usage data:', usageData)
              }
            }

            // æ£€æŸ¥æ˜¯å¦æœ‰é™æµé”™è¯¯
            if (eventData.error && eventData.error.type === 'usage_limit_reached') {
              rateLimitDetected = true
              if (eventData.error.resets_in_seconds) {
                rateLimitResetsInSeconds = eventData.error.resets_in_seconds
                logger.warn(
                  `ðŸš« Rate limit detected in stream, resets in ${rateLimitResetsInSeconds} seconds`
                )
              }
            }
          } catch (e) {
            // å¿½ç•¥è§£æžé”™è¯¯
          }
        }
      }
    }

    upstream.data.on('data', (chunk) => {
      try {
        const chunkStr = chunk.toString()

        // è½¬å‘æ•°æ®ç»™å®¢æˆ·ç«¯
        if (!res.destroyed) {
          res.write(chunk)
        }

        // åŒæ—¶è§£æžæ•°æ®ä»¥æ•èŽ· usage ä¿¡æ¯
        buffer += chunkStr

        // å¤„ç†å®Œæ•´çš„ SSE äº‹ä»¶
        if (buffer.includes('\n\n')) {
          const events = buffer.split('\n\n')
          buffer = events.pop() || '' // ä¿ç•™æœ€åŽä¸€ä¸ªå¯èƒ½ä¸å®Œæ•´çš„äº‹ä»¶

          for (const event of events) {
            if (event.trim()) {
              parseSSEForUsage(event)
            }
          }
        }
      } catch (error) {
        logger.error('Error processing OpenAI stream chunk:', error)
      }
    })

    upstream.data.on('end', async () => {
      // å¤„ç†å‰©ä½™çš„ buffer
      if (buffer.trim()) {
        parseSSEForUsage(buffer)
      }

      // è®°å½•ä½¿ç”¨ç»Ÿè®¡
      if (!usageReported && usageData) {
        try {
          const totalInputTokens = usageData.input_tokens || 0
          const outputTokens = usageData.output_tokens || 0
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0
          // è®¡ç®—å®žé™…è¾“å…¥tokenï¼ˆæ€»è¾“å…¥å‡åŽ»ç¼“å­˜éƒ¨åˆ†ï¼‰
          const actualInputTokens = Math.max(0, totalInputTokens - cacheReadTokens)

          // ä½¿ç”¨å“åº”ä¸­çš„çœŸå®ž modelï¼Œå¦‚æžœæ²¡æœ‰åˆ™ä½¿ç”¨è¯·æ±‚ä¸­çš„ modelï¼Œæœ€åŽå›žé€€åˆ°é»˜è®¤å€¼
          const modelToRecord = actualModel || requestedModel || 'gpt-4'

          await apiKeyService.recordUsage(
            apiKeyData.id,
            actualInputTokens, // ä¼ é€’å®žé™…è¾“å…¥ï¼ˆä¸å«ç¼“å­˜ï¼‰
            outputTokens,
            0, // OpenAIæ²¡æœ‰cache_creation_tokens
            cacheReadTokens,
            modelToRecord,
            accountId
          )

          logger.info(
            `ðŸ“Š Recorded OpenAI usage - Input: ${totalInputTokens}(actual:${actualInputTokens}+cached:${cacheReadTokens}), Output: ${outputTokens}, Total: ${usageData.total_tokens || totalInputTokens + outputTokens}, Model: ${modelToRecord} (actual: ${actualModel}, requested: ${requestedModel})`
          )
          usageReported = true

          await applyRateLimitTracking(
            req,
            {
              inputTokens: actualInputTokens,
              outputTokens,
              cacheCreateTokens: 0,
              cacheReadTokens
            },
            modelToRecord,
            'openai-stream'
          )
        } catch (error) {
          logger.error('Failed to record OpenAI usage:', error)
        }
      }

      // å¦‚æžœåœ¨æµå¼å“åº”ä¸­æ£€æµ‹åˆ°é™æµ
      if (rateLimitDetected) {
        logger.warn(`ðŸš« Processing rate limit for OpenAI account ${accountId} from stream`)
        await unifiedOpenAIScheduler.markAccountRateLimited(
          accountId,
          'openai',
          sessionHash,
          rateLimitResetsInSeconds
        )
      } else if (upstream.status === 200) {
        // æµå¼è¯·æ±‚æˆåŠŸï¼Œæ£€æŸ¥å¹¶ç§»é™¤é™æµçŠ¶æ€
        const isRateLimited = await unifiedOpenAIScheduler.isAccountRateLimited(accountId)
        if (isRateLimited) {
          logger.info(
            `âœ… Removing rate limit for OpenAI account ${accountId} after successful stream`
          )
          await unifiedOpenAIScheduler.removeAccountRateLimit(accountId, 'openai')
        }
      }

      res.end()
    })

    upstream.data.on('error', (err) => {
      logger.error('Upstream stream error:', err)
      if (!res.headersSent) {
        res.status(502).json({ error: { message: 'Upstream stream error' } })
      } else {
        res.end()
      }
    })

    // å®¢æˆ·ç«¯æ–­å¼€æ—¶æ¸…ç†ä¸Šæ¸¸æµ
    const cleanup = () => {
      try {
        upstream.data?.unpipe?.(res)
        upstream.data?.destroy?.()
      } catch (_) {
        //
      }
    }
    req.on('close', cleanup)
    req.on('aborted', cleanup)
  } catch (error) {
    logger.error('Proxy to ChatGPT codex/responses failed:', error)
    // ä¼˜å…ˆä½¿ç”¨ä¸»åŠ¨è®¾ç½®çš„ statusCodeï¼Œç„¶åŽæ˜¯ä¸Šæ¸¸å“åº”çš„çŠ¶æ€ç ï¼Œæœ€åŽé»˜è®¤ 500
    const status = error.statusCode || error.response?.status || 500

    if ((status === 401 || status === 402) && accountId) {
      const statusLabel = status === 401 ? '401é”™è¯¯' : '402é”™è¯¯'
      const extraHint = status === 402 ? 'ï¼Œå¯èƒ½æ¬ è´¹' : ''
      let reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰`
      const errorData = error.response?.data
      if (errorData) {
        if (typeof errorData === 'string' && errorData.trim()) {
          reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${errorData.trim()}`
        } else if (
          errorData.error &&
          typeof errorData.error.message === 'string' &&
          errorData.error.message.trim()
        ) {
          reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${errorData.error.message.trim()}`
        } else if (typeof errorData.message === 'string' && errorData.message.trim()) {
          reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${errorData.message.trim()}`
        }
      } else if (error.message) {
        reason = `OpenAIè´¦å·è®¤è¯å¤±è´¥ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${error.message}`
      }

      try {
        await unifiedOpenAIScheduler.markAccountUnauthorized(
          accountId,
          accountType || 'openai',
          sessionHash,
          reason
        )
      } catch (markError) {
        logger.error('âŒ Failed to mark OpenAI account unauthorized in catch handler:', markError)
      }
    }

    let responsePayload = error.response?.data
    if (!responsePayload) {
      responsePayload = { error: { message: error.message || 'Internal server error' } }
    } else if (typeof responsePayload === 'string') {
      responsePayload = { error: { message: responsePayload } }
    } else if (typeof responsePayload === 'object' && !responsePayload.error) {
      responsePayload = {
        error: { message: responsePayload.message || error.message || 'Internal server error' }
      }
    }

    if (!res.headersSent) {
      res.status(status).json(responsePayload)
    }
  }
}

// æ³¨å†Œä¸¤ä¸ªè·¯ç”±è·¯å¾„ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„å¤„ç†å‡½æ•°
router.post('/responses', authenticateApiKey, handleResponses)
router.post('/v1/responses', authenticateApiKey, handleResponses)
router.post('/responses/compact', authenticateApiKey, handleResponses)
router.post('/v1/responses/compact', authenticateApiKey, handleResponses)

// ä½¿ç”¨æƒ…å†µç»Ÿè®¡ç«¯ç‚¹
router.get('/usage', authenticateApiKey, async (req, res) => {
  try {
    const { usage } = req.apiKey

    res.json({
      object: 'usage',
      total_tokens: usage.total.tokens,
      total_requests: usage.total.requests,
      daily_tokens: usage.daily.tokens,
      daily_requests: usage.daily.requests,
      monthly_tokens: usage.monthly.tokens,
      monthly_requests: usage.monthly.requests
    })
  } catch (error) {
    logger.error('Failed to get usage stats:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve usage statistics',
        type: 'api_error'
      }
    })
  }
})

// API Key ä¿¡æ¯ç«¯ç‚¹
router.get('/key-info', authenticateApiKey, async (req, res) => {
  try {
    const keyData = req.apiKey
    res.json({
      id: keyData.id,
      name: keyData.name,
      description: keyData.description,
      permissions: keyData.permissions || 'all',
      token_limit: keyData.tokenLimit,
      tokens_used: keyData.usage.total.tokens,
      tokens_remaining:
        keyData.tokenLimit > 0
          ? Math.max(0, keyData.tokenLimit - keyData.usage.total.tokens)
          : null,
      rate_limit: {
        window: keyData.rateLimitWindow,
        requests: keyData.rateLimitRequests
      },
      usage: {
        total: keyData.usage.total,
        daily: keyData.usage.daily,
        monthly: keyData.usage.monthly
      }
    })
  } catch (error) {
    logger.error('Failed to get key info:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve API key information',
        type: 'api_error'
      }
    })
  }
})

module.exports = router
module.exports.handleResponses = handleResponses
